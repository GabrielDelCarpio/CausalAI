{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# install.packages(\"tidyverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- \u001b[1mAttaching core tidyverse packages\u001b[22m ------------------------ tidyverse 2.0.0 --\n",
      "\u001b[32mv\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32mv\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
      "\u001b[32mv\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32mv\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32mv\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32mv\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32mv\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32mv\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
      "\u001b[32mv\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n",
      "-- \u001b[1mConflicts\u001b[22m ------------------------------------------ tidyverse_conflicts() --\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mi\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "library('tidyverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data, either from the url or from the path provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m5150\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m21\u001b[39m\n",
      "\u001b[36m--\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m--------------------------------------------------------\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[32mdbl\u001b[39m (21): rownames, wage, lwage, sex, shs, hsg, scl, clg, ad, mw, so, we, ne...\n",
      "\n",
      "\u001b[36mi\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mi\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>5150</li><li>20</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5150\n",
       "\\item 20\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5150\n",
       "2. 20\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5150   20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- read_csv('/Users/gabriel/Documents/GitHub/CausalAI-Course/data/wage2015_subsample_inference.csv')\n",
    "data <- select(data, -rownames)\n",
    "dim(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. As in Group Assignment 1 2024 - 2 #1044 , generate the extra-flexible model. This means that it contains all two-way interactions between the experience polynomials and the indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convertir las variables occ2 e ind2 a factores\n",
    "data$occ2 <- factor(data$occ2)\n",
    "data$ind2 <- factor(data$ind2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X_extra_flexible <- model.matrix(~ sex + (exp1 + exp2 + exp3 + exp4 + hsg + scl + clg + ad + so + we + ne + C(occ2) + C(ind2)) ^ 2, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Generate the array for the outcome variable Y and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "5150"
      ],
      "text/latex": [
       "5150"
      ],
      "text/markdown": [
       "5150"
      ],
      "text/plain": [
       "[1] 5150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y <- data$lwage\n",
    "Y <- (Y-mean(Y))/ sd(Y)\n",
    "Y <- as.vector(Y)\n",
    "length(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Generate the array for the predictors X (do not generate an intercept) and normalize its colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>5150</li><li>980</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5150\n",
       "\\item 980\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5150\n",
       "2. 980\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5150  980"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X <- model.matrix(~ 0 + sex + (exp1 + exp2 + exp3 + exp4 +\n",
    "                    hsg + scl + clg + ad + so + we + ne + C(occ2) + C(ind2))^2, data = data)\n",
    "X <- as.data.frame(X)\n",
    "\n",
    "vars_norm <- c(\"exp1\", \"exp2\", \"exp3\", \"exp4\")\n",
    "\n",
    "for (var in vars_norm) {\n",
    "  X[[var]] <- (X[[var]] - mean(X[[var]])) / sd(X[[var]])\n",
    "}\n",
    "X <- as.matrix(X)\n",
    "dim(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split between training and testing samples. The testing sample should be 10% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>4635</li><li>981</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4635\n",
       "\\item 981\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4635\n",
       "2. 981\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4635  981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>515</li><li>981</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 515\n",
       "\\item 981\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 515\n",
       "2. 981\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 515 981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- cbind(Y, X)\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "data <- as.data.frame(data)\n",
    "train_index <- sample(1:nrow(data), size = 0.9 * nrow(data))\n",
    "train_sample <- data[train_index, ]  # Datos de entrenamiento\n",
    "test_sample <- data[-train_index, ] \n",
    "\n",
    "dim(train_sample)\n",
    "dim(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the Lasso Cross-Validation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: 'Matrix'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:tidyr':\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n",
      "Loaded glmnet 4.1-8\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    lift\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(glmnet) \n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Program a function that generates a logarithmically spaced grid. The input arguments should be the lower and upper bounds of the grid, as well as the natural logarithm of the spacing between each element of the grid. The output should be the logarithmically spaced grid, meaning that if we take the natural logarithm of each entry in the grid, they will be equally spaced. This will be the grid of values for λ values to try during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "log_grid <- function(lower_bound, upper_bound, niter) {\n",
    "  exp(seq(log(lower_bound), log(upper_bound), length.out = niter))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Program a function to generate k folds. It should take as input the array to be split rowwise and the number of folds desired. It should output a list of k 1d arrays of booleans; these arrays should all be the same length as the number of rows in the input array, and when they are all summed together they should add up to an array of all true values. Create your own procedure for splitting. You can aid yourself with third party packages like numpy in Python or Statistics in Julia, but do not use a pre-programmed third party splitting procedure like sk-learns's KFolds in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "generate_folds <- function(Y, k) {\n",
    "  n <- length(Y)\n",
    "  fold_indices <- sample(rep(1:k, length.out = n))\n",
    "  folds <- lapply(1:k, function(i) {\n",
    "    fold <- fold_indices == i\n",
    "    return(fold)\n",
    "  })\n",
    "  return(folds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Program a function that integrates those that you programmed in the last two items to find the value of λ that minimizes the testing mean square error across folds. It should take the following inputs:\n",
    "- Y: an array for the outcome variable.\n",
    "- X: an array of predictors.\n",
    "- lambda_bounds: the lower and upper bounds of the grid of lambda values.\n",
    "- k: number of folds\n",
    "\n",
    "### The output should be a dictionary (a list in R) with the following entries:\n",
    "- optimal_lambda: The lambda that minimizes the testing MSE across folds.\n",
    "- optimal_coef: An array with the coefficients found for the optimal lambda\n",
    "- all_lambdas: The grid of lambdas.\n",
    "- all_mse: An array with the testing MSE across folds for each lambda.\n",
    "\n",
    "### The procedure goes as follows: With each lambda in the grid -> For each split -> Train and test a lasso model.\n",
    "\n",
    "With this, you can get the testing MSE associated with the value of lambda in each iteration. Feel free to use a third party Lasso estimator, as long as it is not one that calculates the optimal lambda value. Instead, the third party estimator you use should only estimate the regression with the penalty weight provided so you can get the testing MSE with that specific weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "optimize_lambda <- function(Y, X, lambda_bounds, k) {\n",
    "  lambdas <- log_grid(lambda_bounds[1], lambda_bounds[2], niter)\n",
    "  folds <- generate_folds(Y, k)\n",
    "  avg_mse <- numeric(length(lambdas))\n",
    "\n",
    "  for (i in seq_along(lambdas)) {\n",
    "    lambda <- lambdas[i]\n",
    "    fold_pe <- numeric(k)\n",
    "\n",
    "    for (j in seq_along(folds)) {\n",
    "      fold <- folds[[j]]\n",
    "      X_train <- X[!fold, ]\n",
    "      Y_train <- Y[!fold]\n",
    "      X_test <- X[fold, ]\n",
    "      Y_test <- Y[fold]\n",
    "\n",
    "      model <- glmnet(X_train, Y_train, alpha = 1, lambda = lambda)\n",
    "      Y_pred <- predict(model, newx = X_test)\n",
    "      fold_pe[j] <- sum((Y_test - Y_pred)^2)\n",
    "    }\n",
    "\n",
    "    avg_mse[i] <- mean(fold_pe)\n",
    "  }\n",
    "\n",
    "  optimal_lambda <- lambdas[which.min(avg_mse)]\n",
    "  optimal_model <- glmnet(X, Y, alpha = 1, lambda = optimal_lambda)\n",
    "\n",
    "  return(list(optimal_lambda = optimal_lambda,\n",
    "              optimal_coef = coef(optimal_model),\n",
    "              model = optimal_model,  # Guardar el modelo aquí\n",
    "              all_lambdas = lambdas,\n",
    "              all_mse = avg_mse))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Program a function for predicting the outcome variable through model estimated with the optimal lambda. It should take as inputs\n",
    "- optimal_model: A dictionary with the values outputed by the function defined for the previous point.\n",
    "- X: an array of predictors.\n",
    "\n",
    "### The output should be an array of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict_outcome <- function(optimal_model, X) {\n",
    "  \n",
    "  lambda_min <- optimal_model$optimal_lambda\n",
    "  predictions <- predict(optimal_model$model, newx = X, s = lambda_min)\n",
    "  \n",
    "  return(predictions)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probando las funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Applying the Lasso Cross-Validation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fit a simple OLS model with the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model_OLS <- lm(Y~., data = train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find the optimal lambda and its related coefficients with the function programmed in 3. and the training sample. Print the lambda and the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Generando los inputs requeridos para que las funciones corran\n",
    "k <- 5\n",
    "lambda_bounds <- c(0.001, 100)\n",
    "niter = 100\n",
    "\n",
    "X_train <- train_sample[, -1]\n",
    "X_train <- as.matrix(X_train)\n",
    "\n",
    "model_opt <- optimize_lambda(train_sample$Y, X_train, lambda_bounds, k)\n",
    "print(model_opt$optimal_lambda)\n",
    "print(model_opt$optimal_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Now use each language's hdm package to fit a Lasso model with the theoretical optimal lambda value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "314.230578367748"
      ],
      "text/latex": [
       "314.230578367748"
      ],
      "text/markdown": [
       "314.230578367748"
      ],
      "text/plain": [
       "[1] 314.2306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "9"
      ],
      "text/latex": [
       "9"
      ],
      "text/markdown": [
       "9"
      ],
      "text/plain": [
       "[1] 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_rlasso <- hdm::rlasso(Y ~ X, post = FALSE) \n",
    "fit_rlasso$lambda0\n",
    "fit_rlasso$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Report the testing MSE and $R^2$ for the OLS model and, the cross-validation lambda Lasso model, and the hdm theoretical lambda model. Use the function programmed in 4. to compute the predictions of the cross-validation Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(model_OLS, newdata = test_sample):\n",
      "\"prediction from a rank-deficient fit may be misleading\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of OLS model is; -4.18676720919766\"\n",
      "[1] \"R square of OLS model is; 5.30772979469966\"\n"
     ]
    }
   ],
   "source": [
    "# Compute the Out-Of-Sample Performance of OLS\n",
    "Y_hat_OLS <- predict(model_OLS, newdata = test_sample)\n",
    "mse_OLS <- mean((test_sample$Y - Y_hat_OLS^2))\n",
    "R_sq_OLS <- 1 - mse_OLS / mean ((test_sample$Y - mean(test_sample$Y))^2)\n",
    "\n",
    "print(paste0(\"MSE of OLS model is; \", mse_OLS))\n",
    "print(paste0(\"R square of OLS model is; \", R_sq_OLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of CV procedure model is; -0.335718491311098\"\n",
      "[1] \"R square of CV procedure model is; 1.3454179502685\"\n"
     ]
    }
   ],
   "source": [
    "# Compute the Out-Of-Sample Performance of Cross validation\n",
    "model_CV <- glmnet(X_train, train_sample$Y, alpha = 1, lambda = model_opt$optimal_lambda)\n",
    "X_test <- test_sample[, -1] \n",
    "Y_hat_CV <- predict(model_CV, newx = as.matrix(X_test))\n",
    "mse_CV <- mean((test_sample$Y - Y_hat_CV^2))\n",
    "R_sq_CV <- 1 - mse_CV / mean ((test_sample$Y - mean(test_sample$Y))^2)\n",
    "\n",
    "print(paste0(\"MSE of CV procedure model is; \", mse_CV))\n",
    "print(paste0(\"R square of CV procedure model is; \", R_sq_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MSE of HDM package model is; -0.0393337311592602\"\n",
      "[1] \"R square of HDM package model is; 1.04047014729628\"\n"
     ]
    }
   ],
   "source": [
    "# Compute the Out-Of-Sample Performance of HDM package\n",
    "model_HDM <- glmnet(X_train, train_sample$Y, alpha = 1, lambda = fit_rlasso$lambda0)\n",
    "Y_hat_HDM <- predict(model_HDM, newx = as.matrix(X_test))\n",
    "mse_HDM <- mean((test_sample$Y - Y_hat_HDM^2))\n",
    "R_sq_HDM <- 1 - mse_HDM / mean ((test_sample$Y - mean(test_sample$Y))^2)\n",
    "\n",
    "print(paste0(\"MSE of HDM package model is; \", mse_HDM))\n",
    "print(paste0(\"R square of HDM package model is; \", R_sq_HDM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
